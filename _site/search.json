[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "",
    "section": "",
    "text": "Project Title: Company Data Integration and Matching\nProject Description: This project seeks to synchronize and consolidate information from two crucial datasets: the Companies file in CRM and the User file in the UCLA ONE platform. The primary objective is to establish a robust connection between the companies listed in both datasets, ensuring data accuracy and completeness. Leveraging the FuzzyWuzzy library, I aim to match company names across both datasets, giving priority to identifying the most popular and frequently occurring company names. The overarching goal is to elevate data quality, streamline company information management, and implement a systematic approach for updating and analyzing the datasets.\nProject Links: Code"
  },
  {
    "objectID": "project.html#ucla-alumni-association-advancement-technology-solutions-project",
    "href": "project.html#ucla-alumni-association-advancement-technology-solutions-project",
    "title": "",
    "section": "",
    "text": "Project Title: Company Data Integration and Matching\nProject Description: This project seeks to synchronize and consolidate information from two crucial datasets: the Companies file in CRM and the User file in the UCLA ONE platform. The primary objective is to establish a robust connection between the companies listed in both datasets, ensuring data accuracy and completeness. Leveraging the FuzzyWuzzy library, I aim to match company names across both datasets, giving priority to identifying the most popular and frequently occurring company names. The overarching goal is to elevate data quality, streamline company information management, and implement a systematic approach for updating and analyzing the datasets.\nProject Links: Code"
  },
  {
    "objectID": "project.html#stats140xp-group-project",
    "href": "project.html#stats140xp-group-project",
    "title": "",
    "section": "STATS140XP Group Project",
    "text": "STATS140XP Group Project\nProject Title: An Analytical Study: Predictive Modeling for the Classification of Flagged Twitter Posts: Insights from Trump’s Tweets Dataset (2009-2021)\nProject Description: In the evolving landscape of social media discourse, the intersection between political communication and platform moderation policies has increased in importance. Using a dataset consisting of web-scraped tweets from Donald Trump between 2009 and 2021, our study aims to predict through a sentiment analysis which tweets are more likely to be flagged by Twitter’s moderation system.\nProject Links: Report, Code, Poster"
  },
  {
    "objectID": "project.html#stats101c-group-project",
    "href": "project.html#stats101c-group-project",
    "title": "",
    "section": "STATS101C Group Project",
    "text": "STATS101C Group Project\nProject Title: IMBD Rating Sentiment Prediction Analysis\nProject Description: If you’re looking for public opinion before selecting a film to watch for movie night, you might want to take a look through IMDb. The website is an online database of information related to films, television series, podcasts, home videos, video games, and streaming content online. In fact, IMDb is the world’s most popular and authoritative source for movie, TV and celebrity content, at least according to. . . IMDb. For our project, we take a look at 50,000 movie reviews pulled from IMDb. The dataset contains one column for the reviews and another with the sentiment, either “positive” or “negative”. Our goal is to analyze the keywords in the reviews and use them to predict the sentiment of each review.\nProject Link: Report, Code"
  },
  {
    "objectID": "project.html#stats-101a-final-project",
    "href": "project.html#stats-101a-final-project",
    "title": "",
    "section": "STATS 101A Final Project",
    "text": "STATS 101A Final Project\nProject Title: Predictors for Baby Weight: A Statistical Analysis\nProject Description: The research question of interest is to identify the best predictors for baby weight among the following factors: gestation period, parity, mother’s age, mother’s height, mother’s weight, and mother’s smoking status. The dataset, named “Pregnancy Data,” was sourced from Kaggle and is based on a study that considered pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area, as stated by Debjeet Das. The paper will commence with a data description of individual variables and an exploration of the relationships between them. Subsequently, it will delve into the examination of several predictive models, justifying the selection of the “best” model. Finally, a concise summary and discussion of the limitations and potential improvements of the analysis will be presented.\nProject Link: Report"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "About Me\n\nI am a senior undergraduate student at UCLA majoring in Statistics and Data Science. Currently employed as a junior data analyst for the UCLA Alumni Association, I bring a strong foundation in data analysis, with proficiency in R, Python, SQL, Tableau, and Excel. Throughout my academic journey, I have excelled in relevant coursework, including Data Analysis and Regression, Design and Analysis of Experiments, Statistical Models and Data Mining, Computational Statistics, Computation and Optimization for Statistics, Monte Carlo Methods, Statistical Consulting, and Geo-statistics, and have received recognition for my commitment to excellence (Dean’s Honor List). Eager to leverage my skills, I am actively seeking entry-level data analyst positions where I can contribute my analytical skills and continue to grow in the field.\nAttached is my current CV \n\n\nEducation\n\nUniversity of California, Los Angeles\n- BS in Statistics and Data Science\n- 09/2022 - 06/2024\n- GPA: 3.97 \n\n\nSkills\n\n\nLanguages: R, Python, SQL, MATLAB\n\nTechnologies: Tableau, Microsoft Excel, Microsoft PowerPoint, LaTeX, Jira\n\nConcepts: Data Analytics, Statistics, Data Modeling, Data Visualization, Machine learning, Data Cleaning, Communication, Databases, Collaboration, Innovation, Attention to Detail, Time Management, Problem-solving, Critical Thinking \n\n\n\nHobbies\n\n\nHolistic Medicine\n\nVolleyball\n\nPilates\n\nRunning\n\nCooking/Baking"
  },
  {
    "objectID": "docs/STATS140 Project/Stats140XP_project.html",
    "href": "docs/STATS140 Project/Stats140XP_project.html",
    "title": "",
    "section": "",
    "text": "Data Cleaning\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file into a DataFrame\ntweets_df = pd.read_csv('tweets_01-08-2021.csv')\n\n# Removing retweets\ntweets_df1 = tweets_df.loc[tweets_df['isRetweet'] == \"f\"].copy()\n\n# Check for any NA values in the DataFrame\nna_check = tweets_df1.isna().any()\nprint(na_check)\n\nid           False\ntext         False\nisRetweet    False\nisDeleted    False\ndevice       False\nfavorites    False\nretweets     False\ndate         False\nisFlagged    False\ndtype: bool\n\n\n\n# Convert 'date' column to datetime format\ntweets_df1['date'] = pd.to_datetime(tweets_df['date'])\n\n# Extract year from the 'date' column\ntweets_df1['year'] = tweets_df1['date'].dt.year\n\n# Calculate total tweets and 't' isFlagged tweets for each year\nyearly_tweets_table = tweets_df1.groupby('year')['isFlagged'].agg(['count', lambda x: (x == 't').sum()]).reset_index()\nyearly_tweets_table.columns = ['year', 'total_tweets', 'isFlagged_t_count']\n\n# Display the resulting table\nprint(yearly_tweets_table)\n\n    year  total_tweets  isFlagged_t_count\n0   2009            56                  0\n1   2010           142                  0\n2   2011           772                  0\n3   2012          3523                  0\n4   2013          8128                  0\n5   2014          5784                  0\n6   2015          7536                  0\n7   2016          4037                  0\n8   2017          2292                  0\n9   2018          3104                  0\n10  2019          4936                  0\n11  2020          6280                250\n12  2021           104                  0\n\n\n\n# Convert 'date' column to datetime format\ntweets_df1['date'] = pd.to_datetime(tweets_df1['date'])\n\n# Subsetting data to only include tweets from year 2020\ntweets_2020_df = tweets_df1[tweets_df1[\"year\"] == 2020].copy()\n\n# Create a month column\ntweets_2020_df['month'] = tweets_2020_df['date'].dt.month\n\n# Calculate total tweets and 't' isFlagged tweets for each month\nmonthly_tweets_table = tweets_2020_df.groupby('month')['isFlagged'].agg(['count', lambda x: (x == 't').sum()]).reset_index()\nmonthly_tweets_table.columns = ['month', 'total_tweets', 'isFlagged_t_count']\n\n# Display the resulting table\nprint(monthly_tweets_table)\n\n    month  total_tweets  isFlagged_t_count\n0       1           397                  0\n1       2           397                  0\n2       3           447                  1\n3       4           379                  0\n4       5           568                  3\n5       6           542                  3\n6       7           353                  0\n7       8           573                  1\n8       9           724                 11\n9      10           878                  4\n10     11           599                170\n11     12           423                 57\n\n\n\n# Visualization\nplt.figure(figsize=(12, 6))\n\n# Plotting total tweets for each month\nplt.bar(monthly_tweets_table['month'] - 0.2, monthly_tweets_table['total_tweets'], width=0.4, label='Total Tweets', color='blue')\n\n# Plotting 't' flagged tweets for each month\nplt.bar(monthly_tweets_table['month'] + 0.2, monthly_tweets_table['isFlagged_t_count'], width=0.4, label=\"Flagged Tweets\", color='orange')\n\n# Adding some labels and title\nplt.xlabel('Month')\nplt.ylabel('Number of Tweets')\nplt.title('Monthly Tweet Activity for 2020')\nplt.xticks(monthly_tweets_table['month'], ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nplt.legend()\n\n# Save the plot as a PNG file\nplt.savefig('monthly_tweet_activity.png')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nExploratory Analysis\n\n# Cleaned Dataset\n# Trump tweets (no retweets and during the year 2020)\n\nclean_2020_df = tweets_2020_df\n\nprint(clean_2020_df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 6280 entries, 1 to 56570\nData columns (total 11 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   id         6280 non-null   int64         \n 1   text       6280 non-null   object        \n 2   isRetweet  6280 non-null   object        \n 3   isDeleted  6280 non-null   object        \n 4   device     6280 non-null   object        \n 5   favorites  6280 non-null   int64         \n 6   retweets   6280 non-null   int64         \n 7   date       6280 non-null   datetime64[ns]\n 8   isFlagged  6280 non-null   object        \n 9   year       6280 non-null   int64         \n 10  month      6280 non-null   int64         \ndtypes: datetime64[ns](1), int64(5), object(5)\nmemory usage: 588.8+ KB\nNone\n\n\n\nflagged_tweets = clean_2020_df[\"isFlagged\"]\n\n# Table displaying the number and percentage of tweets which are flagged versus not flagged\nfrequency_table = flagged_tweets.value_counts()\npercentages = (frequency_table/ len(flagged_tweets)) * 100\n\nflagged_tweets_table = pd.DataFrame({'Frequency': frequency_table, 'Percentage': percentages})\nflagged_tweets_table\n\n\n  \n    \n\n\n\n\n\n\nFrequency\nPercentage\n\n\n\n\nf\n6030\n96.019108\n\n\nt\n250\n3.980892\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nimport matplotlib.pyplot as plt\n\nflag_counts = clean_2020_df['isFlagged'].value_counts()\ncolors = ['#FF9999', '#66B2FF']\nplt.pie(flag_counts, autopct='%1.1f%%', startangle=90, colors=colors)\n\n# Modify the legend labels to include frequency and percentage\nnot_flagged_label = f'Not Flagged - 6030 (96.0%)'\nflagged_label = f'Flagged - 250 (4.0%)'\nplt.legend([not_flagged_label, flagged_label], loc='lower right')  # Position legend at the right bottom corner\n\nplt.axis('equal')\nplt.title('Trump 2020 Flagged Tweets Distribution')\n\n# Save the pie chart as a PNG file\nplt.savefig('flagged_tweets_pie_chart_with_details_lower_right.png')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom matplotlib import style\nstyle.use('ggplot')\nimport re\nimport nltk\nimport numpy as np\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n!pip install imbalanced-learn\n!pip install pillow==9.5\ntweets_df = pd.read_csv('tweets_01-08-2021.csv')\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\nRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\nRequirement already satisfied: numpy&gt;=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\nRequirement already satisfied: scipy&gt;=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\nRequirement already satisfied: scikit-learn&gt;=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.3.0)\nCollecting pillow==9.5\n  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 12.5 MB/s eta 0:00:00\nInstalling collected packages: pillow\n  Attempting uninstall: pillow\n    Found existing installation: Pillow 9.4.0\n    Uninstalling Pillow-9.4.0:\n      Successfully uninstalled Pillow-9.4.0\nSuccessfully installed pillow-9.5.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\n# Convert 'f' and 't' to False and True\ntweets_df['isRetweet'] = tweets_df['isRetweet'].replace({'f': False, 't': True})\n\n# Convert the 'isRetweet' column to boolean type\ntweets_df['isRetweet'] = tweets_df['isRetweet'].astype(bool)\n\n# Convert 'f' and 't' to False and True\ntweets_df['isDeleted'] = tweets_df['isDeleted'].replace({'f': False, 't': True})\n\n# Convert the 'isDeleted' column to boolean type\ntweets_df['isDeleted'] = tweets_df['isDeleted'].astype(bool)\n\n# Convert 'f' and 't' to False and True\ntweets_df['isFlagged'] = tweets_df['isFlagged'].replace({'f': False, 't': True})\n\n# Convert the 'isFlagged' column to boolean type\ntweets_df['isFlagged'] = tweets_df['isFlagged'].astype(bool)\n\n\ntweets_df = tweets_df.loc[tweets_df['isRetweet'] == False]\ntweets_df\n\n\n  \n    \n\n\n\n\n\n\nid\ntext\nisRetweet\nisDeleted\ndevice\nfavorites\nretweets\ndate\nisFlagged\n\n\n\n\n0\n98454970654916608\nRepublicans and Democrats have both created ou...\nFalse\nFalse\nTweetDeck\n49\n255\n2011-08-02 18:07:48\nFalse\n\n\n1\n1234653427789070336\nI was thrilled to be back in the Great city of...\nFalse\nFalse\nTwitter for iPhone\n73748\n17404\n2020-03-03 01:34:50\nFalse\n\n\n3\n1304875170860015617\nThe Unsolicited Mail In Ballot Scam is a major...\nFalse\nFalse\nTwitter for iPhone\n80527\n23502\n2020-09-12 20:10:58\nFalse\n\n\n6\n1223640662689689602\nGetting a little exercise this morning! https:...\nFalse\nFalse\nTwitter for iPhone\n285863\n30209\n2020-02-01 16:14:02\nFalse\n\n\n7\n1319501865625784320\nhttps://t.co/4qwCKQOiOw\nFalse\nFalse\nTwitter for iPhone\n130822\n19127\n2020-10-23 04:52:14\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n56555\n1213078681750573056\nIran never won a war, but never lost a negotia...\nFalse\nFalse\nTwitter for iPhone\n303007\n57253\n2020-01-03 12:44:30\nFalse\n\n\n56559\n1212177432452698115\nThank you to the @dcexaminer Washington Examin...\nFalse\nFalse\nTwitter for iPhone\n35044\n9213\n2020-01-01 01:03:15\nFalse\n\n\n56560\n1212175360093229056\nOne of my greatest honors was to have gotten C...\nFalse\nFalse\nTwitter for iPhone\n56731\n12761\n2020-01-01 00:55:01\nFalse\n\n\n56569\n1319384118849949702\nJust signed an order to support the workers of...\nFalse\nFalse\nTwitter for iPhone\n176289\n36001\n2020-10-22 21:04:21\nFalse\n\n\n56570\n1319345719829008387\nSuburban women want Safety &amp; Security. Joe...\nFalse\nFalse\nTwitter for iPhone\n95169\n19545\n2020-10-22 18:31:46\nFalse\n\n\n\n\n46694 rows × 9 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n## Remove Stop Words and Tokenize the Tweets\" ##\nstop_words = set(stopwords.words('english'))\ndef data_processing(text):\n    text= text.lower()\n    text = re.sub('&lt;br /&gt;', '', text)\n    text = re.sub(r\"https\\S+|www\\S+|http\\S+\", '', text, flags = re.MULTILINE)\n    text = re.sub(r'\\@w+|\\#', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text_tokens = word_tokenize(text)\n    filtered_text = [w for w in text_tokens if not w in stop_words]\n    return \" \".join(filtered_text)\n\n# clean tweets by removing stop words and unecessary text\n\ntweets_df.text = tweets_df['text'].apply(data_processing)\ntweets_df.text.head(6)\n\n0      republicans democrats created economic problems\n1    thrilled back great city charlotte north carol...\n3    unsolicited mail ballot scam major threat demo...\n6                      getting little exercise morning\n7                                                     \n8                                                     \nName: text, dtype: object\n\n\n\n## Stem the words we tokenized from the tweets ##\nstemmer = PorterStemmer()\ndef stemming(data):\n    text = [stemmer.stem(word) for word in data]\n    return data\n# apply stemming to text\ntweets_df.text = tweets_df['text'].apply(lambda x: stemming(x))\n\nX = tweets_df['text']\nY = tweets_df['isFlagged']\n\n\n# Separate the flagged and not flagged tweets to prepare for counter\nflagged_tweets =  tweets_df[tweets_df.isFlagged == True]\nnonflagged_tweets = tweets_df[tweets_df.isFlagged == False]\n\n# Seeing the most commonly used words in flagged tweets\ncount = Counter()\nfor text in flagged_tweets['text'].values:\n    for word in text.split():\n        count[word] +=1\ncount.most_common(15)\n\n# seeing the most commonly used words in nonflagged tweets\nfor text in nonflagged_tweets['text'].values:\n    for word in text.split():\n        count[word] +=1\ncount.most_common(15)\n\n[('realdonaldtrump', 8405),\n ('great', 7201),\n ('trump', 5275),\n ('amp', 4930),\n ('thank', 3307),\n ('president', 3102),\n ('people', 3044),\n ('us', 2406),\n ('would', 2199),\n ('get', 2180),\n ('country', 2114),\n ('new', 2112),\n ('thanks', 2082),\n ('big', 1994),\n ('america', 1890)]\n\n\n\n# IMPORTANT FOR BALANCING\nfrom imblearn.over_sampling import (RandomOverSampler)\n\nvect = TfidfVectorizer()\nX = vect.fit_transform(tweets_df['text'])\n\nresamp = RandomOverSampler()\nX, Y = resamp.fit_resample(X,Y)\n\n\n# Dimension reduction using PCA\nfrom sklearn.decomposition import TruncatedSVD\npca = TruncatedSVD(n_components = 1000)\nX_pca = pca.fit_transform(X)\n\n\n# split dataset into training (80%) and testing (20%)\nx_train, x_test, y_train, y_test = train_test_split(X_pca, Y, test_size = 0.2, random_state = 123)\nprint(\"Size of x_train: \", (x_train.shape))\nprint(\"Size of y_train: \", (y_train.shape))\nprint(\"Size of x_test: \", (x_test.shape))\nprint(\"Size of y_test: \", (y_test.shape))\n\nSize of x_train:  (74310, 1000)\nSize of y_train:  (74310,)\nSize of x_test:  (18578, 1000)\nSize of y_test:  (18578,)\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n## naive bayes ##\nx_train_nb, x_test_nb, y_train_nb, y_test_nb = train_test_split(X, Y, test_size = 0.2, random_state = 123)\nprint(\"Size of x_train_nb: \", (x_train_nb.shape))\nprint(\"Size of y_train_nb: \", (y_train_nb.shape))\nprint(\"Size of x_test_nb: \", (x_test_nb.shape))\nprint(\"Size of y_test_nb: \", (y_test_nb.shape))\n\n\nmnb = MultinomialNB()\nmnb.fit(x_train_nb, y_train_nb)\nmnb_pred = mnb.predict(x_test_nb)\nmnb_acc = accuracy_score(mnb_pred, y_test_nb)\nprint(\"Test accuracy: {:.2f}%\".format(mnb_acc*100))\nprint(classification_report(y_test_nb, mnb_pred))\n\nSize of x_train_nb:  (74310, 43806)\nSize of y_train_nb:  (74310,)\nSize of x_test_nb:  (18578, 43806)\nSize of y_test_nb:  (18578,)\nTest accuracy: 97.39%\n              precision    recall  f1-score   support\n\n       False       1.00      0.95      0.97      9356\n        True       0.95      1.00      0.97      9222\n\n    accuracy                           0.97     18578\n   macro avg       0.98      0.97      0.97     18578\nweighted avg       0.98      0.97      0.97     18578\n\n\n\n\n## Logistic Regression ##\nlogreg = LogisticRegression(solver = \"lbfgs\")\nlogreg.fit(x_train, y_train)\nlogreg_pred = logreg.predict(x_test)\nlogreg_acc = accuracy_score(logreg_pred, y_test)\nprint(\"Test accuracy: {:.2f}%\".format(logreg_acc*100))\nprint(classification_report(y_test, logreg_pred))\n\nTest accuracy: 97.32%\n              precision    recall  f1-score   support\n\n       False       1.00      0.95      0.97      9356\n        True       0.95      1.00      0.97      9222\n\n    accuracy                           0.97     18578\n   macro avg       0.97      0.97      0.97     18578\nweighted avg       0.97      0.97      0.97     18578\n\n\n\n\n## Linear SVC ##\nsvc = LinearSVC(C = 100, loss = 'squared_hinge')\nsvc.fit(x_train, y_train)\nsvc_pred = svc.predict(x_test)\nsvc_acc = accuracy_score(svc_pred, y_test)\nprint(\"Test accuracy: {:.2f}%\".format(svc_acc*10 and I 0))\nprint(classification_report(y_test, svc_pred))\n\nTest accuracy: 97.86%\n              precision    recall  f1-score   support\n\n       False       1.00      0.96      0.98      9356\n        True       0.96      1.00      0.98      9222\n\n    accuracy                           0.98     18578\n   macro avg       0.98      0.98      0.98     18578\nweighted avg       0.98      0.98      0.98     18578\n\n\n\n\n## Using GridSearch to find the best hyperparameters for our SVC Model ##\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C':[0.1, 1, 10, 100], 'loss':['hinge', 'squared_hinge']}\ngrid = GridSearchCV(svc, param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train)\n\nprint(\"best cross validation score: {:.3f}\".format(grid.best_score_))\nprint(\"best parameters: \", grid.best_params_)\n\nFitting 5 folds for each of 8 candidates, totalling 40 fits\n[CV 1/5] END .................C=0.1, loss=hinge;, score=0.905 total time=  11.2s\n[CV 2/5] END .................C=0.1, loss=hinge;, score=0.902 total time=  11.2s\n[CV 3/5] END .................C=0.1, loss=hinge;, score=0.901 total time=  16.5s\n[CV 4/5] END .................C=0.1, loss=hinge;, score=0.900 total time=  15.8s\n[CV 5/5] END .................C=0.1, loss=hinge;, score=0.902 total time=  14.6s\n[CV 1/5] END .........C=0.1, loss=squared_hinge;, score=0.975 total time=   3.5s\n[CV 2/5] END .........C=0.1, loss=squared_hinge;, score=0.974 total time=   3.8s\n[CV 3/5] END .........C=0.1, loss=squared_hinge;, score=0.976 total time=   4.3s\n[CV 4/5] END .........C=0.1, loss=squared_hinge;, score=0.975 total time=   3.2s\n[CV 5/5] END .........C=0.1, loss=squared_hinge;, score=0.976 total time=   3.5s\n[CV 1/5] END ...................C=1, loss=hinge;, score=0.972 total time= 1.4min\n[CV 2/5] END ...................C=1, loss=hinge;, score=0.971 total time=  38.4s\n[CV 3/5] END ...................C=1, loss=hinge;, score=0.972 total time= 1.4min\n[CV 4/5] END ...................C=1, loss=hinge;, score=0.971 total time= 1.4min\n[CV 5/5] END ...................C=1, loss=hinge;, score=0.973 total time= 1.5min\n[CV 1/5] END ...........C=1, loss=squared_hinge;, score=0.975 total time=   7.8s\n[CV 2/5] END ...........C=1, loss=squared_hinge;, score=0.975 total time=   6.8s\n[CV 3/5] END ...........C=1, loss=squared_hinge;, score=0.976 total time=   7.5s\n[CV 4/5] END ...........C=1, loss=squared_hinge;, score=0.976 total time=   7.2s\n[CV 5/5] END ...........C=1, loss=squared_hinge;, score=0.977 total time=   7.1s\n[CV 1/5] END ..................C=10, loss=hinge;, score=0.977 total time= 1.5min\n[CV 2/5] END ..................C=10, loss=hinge;, score=0.977 total time= 1.5min\n[CV 3/5] END ..................C=10, loss=hinge;, score=0.978 total time= 1.5min\n[CV 4/5] END ..................C=10, loss=hinge;, score=0.978 total time= 1.5min\n[CV 5/5] END ..................C=10, loss=hinge;, score=0.978 total time= 1.5min\n[CV 1/5] END ..........C=10, loss=squared_hinge;, score=0.978 total time= 1.0min\n[CV 2/5] END ..........C=10, loss=squared_hinge;, score=0.977 total time=  57.0s\n[CV 3/5] END ..........C=10, loss=squared_hinge;, score=0.978 total time= 1.1min\n[CV 4/5] END ..........C=10, loss=squared_hinge;, score=0.978 total time=  52.7s\n[CV 5/5] END ..........C=10, loss=squared_hinge;, score=0.979 total time=  56.3s\n[CV 1/5] END .................C=100, loss=hinge;, score=0.979 total time= 1.6min\n[CV 2/5] END .................C=100, loss=hinge;, score=0.978 total time= 1.6min\n[CV 3/5] END .................C=100, loss=hinge;, score=0.978 total time= 1.6min\n[CV 4/5] END .................C=100, loss=hinge;, score=0.978 total time= 1.5min\n[CV 5/5] END .................C=100, loss=hinge;, score=0.980 total time= 1.6min\n[CV 1/5] END .........C=100, loss=squared_hinge;, score=0.980 total time= 1.7min\n[CV 2/5] END .........C=100, loss=squared_hinge;, score=0.978 total time= 1.7min\n[CV 3/5] END .........C=100, loss=squared_hinge;, score=0.979 total time= 1.6min\n[CV 4/5] END .........C=100, loss=squared_hinge;, score=0.978 total time= 1.6min\n[CV 5/5] END .........C=100, loss=squared_hinge;, score=0.980 total time= 1.5min\nbest cross validation score: 0.979\nbest parameters:  {'C': 100, 'loss': 'squared_hinge'}\n\n\n\n## random forest ##\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\nrf_pred = rf.predict(x_test)\nrf_acc = accuracy_score(rf_pred, y_test)\nprint(\"Training set score: {:.3f}\".format(rf.score(x_train, y_train)))\nprint(\"Test accuracy: {:.2f}%\".format(rf_acc * 100))\nprint(classification_report(y_test, rf_pred))\n\nTraining set score: 0.986\nTest accuracy: 98.53%\n              precision    recall  f1-score   support\n\n       False       1.00      0.97      0.99      9356\n        True       0.97      1.00      0.99      9222\n\n    accuracy                           0.99     18578\n   macro avg       0.99      0.99      0.99     18578\nweighted avg       0.99      0.99      0.99     18578\n\n\n\n\n## knn ##\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train, y_train)\ny_pred_knn = knn.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred_knn)\n\nprint(\"Accuracy:\", accuracy)\nprint(classification_report(y_test, y_pred_knn))\n\nAccuracy: 0.9849822370545807\n              precision    recall  f1-score   support\n\n       False       1.00      0.97      0.98      9356\n        True       0.97      1.00      0.99      9222\n\n    accuracy                           0.98     18578\n   macro avg       0.99      0.99      0.98     18578\nweighted avg       0.99      0.98      0.98     18578\n\n\n\n\n#Feature index Plot\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming 'rf' is your trained RandomForestClassifier\nimportances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), indices)\nplt.xlim([-1, x_train.shape[1]])\nplt.xlabel('Feature Index')\nplt.ylabel('Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nconf_mat = confusion_matrix(y_test, rf.predict(x_test))\nsns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n#ROC and AOC\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Compute ROC curve and AUC for a binary classification task\nfpr, tpr, thresholds = roc_curve(y_test, rf.predict_proba(x_test)[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#Precision recall curve\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\nprecision, recall, _ = precision_recall_curve(y_test, rf.predict_proba(x_test)[:, 1])\naverage_precision = average_precision_score(y_test, rf.predict_proba(x_test)[:, 1])\n\nplt.figure()\nplt.step(recall, precision, where='post', label='Average precision (AP)={0:0.2f}'.format(average_precision))\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\nplt.legend(loc=\"upper right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSize of x_train_knn:  (74310, 43806)\nSize of y_train_knn:  (74310,)\nSize of x_test_knn:  (18578, 43806)\nSize of y_test_knn:  (18578,)\nAccuracy: 0.5511895790720207\n              precision    recall  f1-score   support\n\n       False       1.00      0.11      0.20      9356\n        True       0.53      1.00      0.69      9222\n\n    accuracy                           0.55     18578\n   macro avg       0.76      0.55      0.44     18578\nweighted avg       0.76      0.55      0.44     18578"
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html",
    "href": "docs/STATS 101C Project/101C_Final_Project.html",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "",
    "text": "If you’re looking for public opinion before selecting a film to watch for movie night, you might want to take a look through IMDb. The website is an online database of information related to films, television series, podcasts, home videos, video games, and streaming content online. In fact, IMDb is the world’s most popular and authoritative source for movie, TV and celebrity content, at least according to…IMDb. Key features of the website include:\n\nMovie/TV Show Listings\nActor & Crew Profiles\nUser Ratings & Reviews\nRelease Dates\nAwards information, etc.\n\nFor our project, we take a look at 50,000 movie reviews pulled from IMDb. The dataset contains one column for the reviews and another with the sentiment, either “positive” or “negative”. Our goal is to analyze the keywords in the reviews and use them to predict the sentiment of each review."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#lexicon-and-etl",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#lexicon-and-etl",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "Lexicon and ETL",
    "text": "Lexicon and ETL\nTo conduct the binary sentiment classification task on the IMDB movie reviews, we make use of Kaggle lexicons made available to us1. We start by loading the IMDB review CSV file as well as the negative and positive lexicons. To better prepare the IMDB review dataset for transformation, we removed some remaining HTML formatting (line breaks: &lt;br /&gt;&lt;br /&gt;) from the reviews. We also added a review_num column to simplify the process of working with the dataset.\nThe negative and positive lexicons each only contained a list of words so we added a column to both lexicons that contained the corresponding sentiments (“positive” or “negative”), then we merged the two lexicons into one larger opinion_lexicon.\nAfter preparing our lexicon, we break down all our reviews into its individual words, grouped by review number. Each row in this new dataset represents one word from one review. Using this new data frame, we also quickly calculated the total word count for each review. This would be used later for Term Frequency(TF) calculations. Then came a crucial step: taking this data frame of review words and performing an inner join to our opinion lexicon words. This filters our data frame of reviews broken down by word for only the words that are in our opinion lexicon. This is huge because it saves us from having to worry about removing any personal pronouns, determiners, coordinating conjunctions, and prepositions from the reviews. After performing this step, we take a deeper look at our new data frame using a frequency chart and word cloud:\nWe can see that the most common word is ‘like’, showing up over 33% more times than the next most common word. Other frequent words include “good”, “well”, “bad”, and “great”. We don’t see any stop words among our most frequent words, indicating a successful dataframe join."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#tf-idf-and-numericalization",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#tf-idf-and-numericalization",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "TF-IDF and Numericalization",
    "text": "TF-IDF and Numericalization\nAlthough our reviews are now filtered to only include words from the lexicon, we still need to remove some more words; those that don’t give us much useful information about the sentiment of the reviews. We have over 5800 unique words from the lexicon in our dataset and utilizing them all as features will be far too computationally expensive.\nThis is where Term Frequency-Inverse Document Frequency(TF-IDF) comes into play. We use this to perform dimensionality reduction on our dataset and remove words outside of a set bound. For the Term Frequency calculation, rather than finding the # of times a word appears in a review / # of words in the review, we found the # of times each word appeared in the entire dataset / the # of words in the entire dataset. Doing this essentially gives us an average TF for each word and limits us to only one TF-IDF for each unique word rather than 50,000.\nThe TF-IDF values ranged from 1.1e-5 to about 0.0315. The vast majority of the 5817 lexicon words, almost 5400 words, had a TF-IDF below 0.002. Since only a small portion of our words had a TF-IDF above this, we decided to make this a minimum cutoff. We did not need a maximum cutoff for the TF-IDF since the stop words had already been taken care of. When filtering for words with a TF-IDF &gt;0.002, we are left with 435 words or “important features”.\nWith these remaining words, we get a count of how many times each word appears in each review, then we pivot this data frame so that each column is one of 435 words, each row is a review number, and each cell value is the frequency of that word in the review. This new numericalized data frame is combined with the columns from our original dataset (excluding the review column) to create our final dataset. 71 of the 50,000 reviews did not contain any of the 435 words so the NA values in those rows were replaced with zeros.\nThe last step in our preprocessing work was to split our dataset of 50k rows and 437 columns (1 row_num + 1 sentiment + 435 words) into testing and training sets of 25k rows each. We now have balanced, numericalized datasets ready for modeling."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#logistic-regression",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#logistic-regression",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a discriminative model often employed for classification problems, much like this one where we are attempting to classify whether a movie review had a “positive” or “negative” sentiment based on the contents of the review.\nThe first step of employing logistic regression was a slight alteration of the data. We had to convert the “sentiment” variable from a character denoting “positive” and “negative” to an integer – 1 for “positive” and 0 for “negative.” This allowed us to employ the function to actually do the logistic regression.\nAfter conducting the logistic regression on the training data, we were able to apply it to the testing data to predict the sentiment of each review in the testing data. These were the results of the data:\nAs we can see, the model was better at accurately predicting the positive reviews than the negative. It had an accuracy of 85.9% for the positive reviews, against just an 81.8% accuracy for the negative reviews. Overall, the logistic regression yielded a total accuracy of 83.8%.\n\nCross Validation and Regularization\nAfter conducting the initial logistic regression on the training and testing datasets, we wanted to make sure that our model did not overfit to the training data and can generalize well to other, new data. To do this, we looked to L2 (Ridge) Regularization. To determine the parameter, lambda, which controls the strength of the penalty term, we used 5-fold cross validation. Interestingly, the cv kept selecting the smallest \\(\\lambda\\) in our range as the best choice, even as we continued to feed the cv model a lower range. It became clear the the best results/accuracy came from a model with no penalty. In fact, when using 5-fold cv directly on our training dataset with no penalty term, we get an accuracy of 83.9%, slightly higher than the initial model. This tells us that our model is stable and generalizes well across different subsets of data.\nThe ROC curve for logistic regression shows a relatively large area under the curve, indicating that the model fits well."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#linear-discriminant-analysis",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#linear-discriminant-analysis",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "Linear Discriminant Analysis",
    "text": "Linear Discriminant Analysis\nLinear Discriminant Analysis (LDA) is a generative model, unlike the discriminative logistic model, which studies the joint probability distribution as opposed to the purely conditional probability in generative models.\nFirst, we must make an assumption that the conditional probability of the review sentiment based on the words found in the review follows a multivariate normal distribution with a fixed mean vector and covariance matrix. This step of the process will also be repeated when we conduct Quadratic Discriminant Analysis (QDA) later. However, the main difference between the LDA and the QDA is that here, we will assume that the classes have a common covariance matrix between them.\nThe methods of conducting LDA are largely the same as the logistic regression, as we use the same data set which alters “positive” and “negative” to 1’s and 0’s, and the process of generating the table is virtually the same by using the training data to predict the testing data.\nThe accuracy of predicting a positive sentiment on the testing data was 81.9%, and the accuracy of predicting a negative sentiment on the testing data was 85.2%, so compared to the logistic regression, both the true positive rate and true negative rate increased, and the true negative rate was higher than the true positive rate. The overall accuracy of this model is about 83.5%, which is an improvement from the logistic regression model.\nThis is the Receiver Operating Characteristic (ROC) curve for the LDA model, and what this helps visualize are the true positive and false positive rates. The more bowed out the curve is, the better the model performed in terms of accuracy, and the one of the LDA model is very bowed out, with a large area underneath the curve, so we can assume this model was strong in predicting the sentiment of the review based on the words in the review."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#quadratic-discriminant-analysis",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#quadratic-discriminant-analysis",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "Quadratic Discriminant Analysis",
    "text": "Quadratic Discriminant Analysis\nQuadratic Discriminant Analysis (QDA), another type of generative model, is closely related to LDA. QDA retains one of the core LDA assumptions that the observations within each class are drawn from a multivariate Gaussian distribution. However, as stated previously, an important distinction that differentiates QDA from LDA is that each class has its own covariance matrix. Typically, QDA is utilized when the training dataset is very large so that the variance of the classifier is not a major concern. QDA serves as a middle ground between non-parametric methods like KNN and linear approaches like LDA and logistic regression. While it assumes a quadratic decision boundary, it still provides more flexibility than the linear methods, giving it the potential to capture more complex relationships2.\nAfter training the QDA model on the training dataset of 25,000 samples, we evaluated its performance on the testing data, which consists of the same amount of samples. The response variable Sentiment was converted into a factor before fitting the model and making predictions, with levels “Positive” and “Negative,” to ensure that it is treated as a binary classification variable.\nAs we can see from the confusion matrix, the accuracy of predicting a positive sentiment is around 78.3% (True Positives/Total Positives), while the accuracy of predicting a negative sentiment is around 82.3% (True Negatives/Total Negatives). These accuracies are both lower than QDA’s linear counterpart LDA’s TP and TN accuracies. The overall accuracy of the QDA model is 80.2%, slightly less than both Logistic Regression and LDA.\nA ROC curve plots the True Positive Rate (TPR) vs False Positive Rate (FPR) at different classification thresholds. “Sensitivity” refers to the TPR, whereas 1 - Specificity refers to the FPR. A side-by-side comparison of the QDA Model ROC to those of Logistic Regression and LDA shows that both models outperform QDA; their curves are more bowed out towards the top-left corner, indicating a higher sensitivity (TPR)3.\n\nWhy does Logistic Regression and LDA outperform QDA?\nAs we saw from the results, LDA and Logistic Regression slightly outperform QDA. While the difference may be marginal, there are possible explanations that might help us understand the differing accuracies. The assumption of varying covariance matrices for different classes may introduce higher variance, leading to possible overfitting and reduced generalization ability. The comparatively simpler LDA and logistic regression models might find it easier to generalize, underscoring a potential bias-variance tradeoff that favors these models."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#random-forest",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#random-forest",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "Random Forest",
    "text": "Random Forest\nDecision trees are constructed by repeated splits of subsets of the data (the root node) into two descending subsets (child nodes). A terminal node, or leaf node, ideally occurs when that node is purely made up of one class. By combining hundreds or even thousands of decision trees, we can create a random forest that contains trees with many different subsets of the data. Random forest models are considered one of the more accurate predictive models, although the computing time is quite long and inefficient compared to other models (i.e. this algorithm is computationally expensive).\nAfter splitting the dataset into 50% for training and 50% for testing, we first did Random Search Cross Validation in order to determine some ideal hyperparameters for our model. The results suggested the maximum number of features for each tree be 4.\nOur random forest model with the updated hyperparameter had an accuracy of 83.4%, which was roughly 2% better than the model with default hyperparameters. Shown below are the results of the model and its ROC curve.\nThe true positive rate (sensitivity) was 85.1%, and the false positive rate (specificity) was 81.8%."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#future-analysis",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#future-analysis",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "Future Analysis",
    "text": "Future Analysis\nWhile we did end up evaluating four different modeling techniques, future iterations of this study could include a K nearest neighbors analysis, or perform K-fold cross validation on the models other than Logistic Regression and Random Forest. However, cross validation would have made the models even more computationally expensive. In addition, KNN likely would not have performed as well as some of the models we implemented. KNN is a very flexible model and it likely would have overfit to the training data like QDA.\nOnce again, our models performed fairly well, especially considering that we only used an opinion lexicon and term frequencies to numericalize our reviews. If we were looking to improve our accuracy, possibly north of 90%, we could have also accounted for the context surrounding words. This would help us determine if a word really reflected positive or negative sentiment. In addition, we could have taken punctuation, such exclamation marks, and capitalized words into account. Although relatively uncommon and difficult to detect, the presence of sarcasm could also be sought out. This would also make us reconsider the true sentiment a word or sentence displays.\nLastly, keeping more words from our lexicon would have helped us in improving the accuracy of our models. Reducing our unique words to 435 did ensure that our models ran more efficiently but the TF-IDF cutoff used to make that selection was arbitrarily selected. This reduction also led to some reviews no longer having any words that were “important features”. Keeping more words, at least enough that every review has a word, would be a good decision if we were to perform this analysis again."
  },
  {
    "objectID": "docs/STATS 101C Project/101C_Final_Project.html#footnotes",
    "href": "docs/STATS 101C Project/101C_Final_Project.html#footnotes",
    "title": "IMDB Rating Sentiment Prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLexicon was provided by professor and can be found here↩︎\nSource: ISLR Textbook pg. 151↩︎\nSource: Classification: ROC curve and AUC↩︎"
  },
  {
    "objectID": "work_experience.html",
    "href": "work_experience.html",
    "title": "",
    "section": "",
    "text": "UCLA Alumni Association Advancement Technology Solutions July 2023 – Present  Junior Data Analyst Remote  * Collaborated on over 15 diverse data analysis and extraction projects, collaborating closely with clients and demonstrating expertise in utilizing Python and SQL for data extraction, cleansing, and analysis from CRM databases.  * Utilized Tableau and PowerPoint to transform complex data into actionable insights, creating compelling visualizations for clients.  * Enhanced team efficiency by providing guidance and training to a fellow student worker in event attendance processing.  * Conducted thorough reviews of over 400 event attendance workbooks, ensuring data completeness, cleanliness, and proper formatting.  * Proactively identified and resolved data discrepancies, collaborating closely with data senders to address complex issues. \nAcademy Volleyball Club June 2018 – Sept 2023  Head Volleyball Coach Redwood City, CA  * Led 3 nationally ranked girls’ volleyball teams across various age groups.  * Took charge of organizing camp schedules and planned engaging camp activities to enhance players’ skills and create a positive learning environment.  * Maintained a commitment of approximately 20 hours per week.\nNextUp Ventures Mar 2023 – July 2023  Marketing Analyst Intern Remote  * Conducted in-depth sector-by-sector analyses of target markets within key geographies.  * Generated innovative ideas and led social media targeting initiatives for key markets, executing effective strategies to enhance brand visibility and engagement.  * Proficiently organized impactful remote events for founders in key geographies, creating valuable networking and collaboration opportunities.\nStanford Genome Technology Center Dec 2021 – Mar 2022  Intern Under the Director of Translational Medicine Ami Mac Palo Alto, CA  * Actively engaged in board meetings, gaining comprehensive insights into the diverse impacts of Long-COVID and Chronic Fatigue Syndrome on various bodily systems.  * Performed a thorough review of existing research articles on Long-COVID and Chronic Fatigue Syndrome. Summarized key findings, contributing valuable insights to the research project.  * Contributed to an ongoing longitudinal study, utilizing collected data to assess functional outcomes in patients over time."
  },
  {
    "objectID": "work_experience.html#work-experience",
    "href": "work_experience.html#work-experience",
    "title": "",
    "section": "",
    "text": "UCLA Alumni Association Advancement Technology Solutions July 2023 – Present  Junior Data Analyst Remote  * Collaborated on over 15 diverse data analysis and extraction projects, collaborating closely with clients and demonstrating expertise in utilizing Python and SQL for data extraction, cleansing, and analysis from CRM databases.  * Utilized Tableau and PowerPoint to transform complex data into actionable insights, creating compelling visualizations for clients.  * Enhanced team efficiency by providing guidance and training to a fellow student worker in event attendance processing.  * Conducted thorough reviews of over 400 event attendance workbooks, ensuring data completeness, cleanliness, and proper formatting.  * Proactively identified and resolved data discrepancies, collaborating closely with data senders to address complex issues. \nAcademy Volleyball Club June 2018 – Sept 2023  Head Volleyball Coach Redwood City, CA  * Led 3 nationally ranked girls’ volleyball teams across various age groups.  * Took charge of organizing camp schedules and planned engaging camp activities to enhance players’ skills and create a positive learning environment.  * Maintained a commitment of approximately 20 hours per week.\nNextUp Ventures Mar 2023 – July 2023  Marketing Analyst Intern Remote  * Conducted in-depth sector-by-sector analyses of target markets within key geographies.  * Generated innovative ideas and led social media targeting initiatives for key markets, executing effective strategies to enhance brand visibility and engagement.  * Proficiently organized impactful remote events for founders in key geographies, creating valuable networking and collaboration opportunities.\nStanford Genome Technology Center Dec 2021 – Mar 2022  Intern Under the Director of Translational Medicine Ami Mac Palo Alto, CA  * Actively engaged in board meetings, gaining comprehensive insights into the diverse impacts of Long-COVID and Chronic Fatigue Syndrome on various bodily systems.  * Performed a thorough review of existing research articles on Long-COVID and Chronic Fatigue Syndrome. Summarized key findings, contributing valuable insights to the research project.  * Contributed to an ongoing longitudinal study, utilizing collected data to assess functional outcomes in patients over time."
  },
  {
    "objectID": "work_experience.html#leadershipcampus-involvement",
    "href": "work_experience.html#leadershipcampus-involvement",
    "title": "",
    "section": "Leadership/Campus Involvement",
    "text": "Leadership/Campus Involvement\nUCLA Women’s Indoor Volleyball Club Sept 2023 - Present  Treasurer Palo Alto, CA  * Implemented effective budgeting strategies to ensure financial stability of the club.  * Managed and monitored club finances, tracking income and expenses with precision using Excel.  * Designed and implemented inventive fundraising events, including a grass volleyball tournament, leading to a significant revenue increase of over $2000.  * Facilitated reimbursement processes for club members, promoting financial accountability.\nUCSD Women’s Volleyball Team Aug 2020 - June 2022  Division 1 Volleyball Athlete Palo Alto, CA  * Played as a Division 1 Volleyball Athlete for a duration of 2 years.  * Demonstrated dedication by allocating 20 hours per week to athletic commitments while successfully managing a full academic course load.  * Acquired exceptional time management and teamwork skills."
  }
]