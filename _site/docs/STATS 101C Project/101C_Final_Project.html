<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.541">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ayo Ayiloge, Odeya Russo, Joseph Crosby, Caitlin Ree, Suraj Rajan, Wyatt Stone">
<meta name="author" content="UCLA Department of Statistics and Data Science">
<meta name="dcterms.date" content="2023-01-01">

<title>IMDB Rating Sentiment Prediction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-fixed">\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../work_experience.html"> 
<span class="menu-text">Experience</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../project.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#pre-processing-and-eda" id="toc-pre-processing-and-eda" class="nav-link" data-scroll-target="#pre-processing-and-eda">Pre-Processing and EDA</a>
  <ul class="collapse">
  <li><a href="#lexicon-and-etl" id="toc-lexicon-and-etl" class="nav-link" data-scroll-target="#lexicon-and-etl">Lexicon and ETL</a></li>
  <li><a href="#tf-idf-and-numericalization" id="toc-tf-idf-and-numericalization" class="nav-link" data-scroll-target="#tf-idf-and-numericalization">TF-IDF and Numericalization</a></li>
  </ul></li>
  <li><a href="#analysis" id="toc-analysis" class="nav-link" data-scroll-target="#analysis">Analysis</a>
  <ul class="collapse">
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#cross-validation-and-regularization" id="toc-cross-validation-and-regularization" class="nav-link" data-scroll-target="#cross-validation-and-regularization">Cross Validation and Regularization</a></li>
  </ul></li>
  <li><a href="#linear-discriminant-analysis" id="toc-linear-discriminant-analysis" class="nav-link" data-scroll-target="#linear-discriminant-analysis">Linear Discriminant Analysis</a></li>
  <li><a href="#quadratic-discriminant-analysis" id="toc-quadratic-discriminant-analysis" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis">Quadratic Discriminant Analysis</a>
  <ul class="collapse">
  <li><a href="#why-does-logistic-regression-and-lda-outperform-qda" id="toc-why-does-logistic-regression-and-lda-outperform-qda" class="nav-link" data-scroll-target="#why-does-logistic-regression-and-lda-outperform-qda">Why does Logistic Regression and LDA outperform QDA?</a></li>
  </ul></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">Random Forest</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#future-analysis" id="toc-future-analysis" class="nav-link" data-scroll-target="#future-analysis">Future Analysis</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>IMDB Rating Sentiment Prediction</strong></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Ayo Ayiloge, Odeya Russo, Joseph Crosby, Caitlin Ree, Suraj Rajan, Wyatt Stone </p>
             <p>UCLA Department of Statistics and Data Science </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2023</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>If you’re looking for public opinion before selecting a film to watch for movie night, you might want to take a look through IMDb. The website is an online database of information related to films, television series, podcasts, home videos, video games, and streaming content online. In fact, IMDb is the world’s most popular and authoritative source for movie, TV and celebrity content, at least according to…IMDb. Key features of the website include:</p>
<ol type="1">
<li>Movie/TV Show Listings</li>
<li>Actor &amp; Crew Profiles</li>
<li>User Ratings &amp; Reviews</li>
<li>Release Dates</li>
<li>Awards information, etc.</li>
</ol>
<p>For our project, we take a look at 50,000 movie reviews pulled from IMDb. The dataset contains one column for the reviews and another with the sentiment, either “positive” or “negative”. Our goal is to analyze the keywords in the reviews and use them to predict the sentiment of each review.</p>
</section>
<section id="pre-processing-and-eda" class="level1">
<h1>Pre-Processing and EDA</h1>
<section id="lexicon-and-etl" class="level2">
<h2 class="anchored" data-anchor-id="lexicon-and-etl">Lexicon and ETL</h2>
<p>To conduct the binary sentiment classification task on the IMDB movie reviews, we make use of Kaggle lexicons made available to us<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. We start by loading the IMDB review CSV file as well as the negative and positive lexicons. To better prepare the IMDB review dataset for transformation, we removed some remaining HTML formatting (line breaks: <code>&lt;br /&gt;&lt;br /&gt;</code>) from the reviews. We also added a review_num column to simplify the process of working with the dataset.</p>
<p>The negative and positive lexicons each only contained a list of words so we added a column to both lexicons that contained the corresponding sentiments (“positive” or “negative”), then we merged the two lexicons into one larger opinion_lexicon.</p>
<p>After preparing our lexicon, we break down all our reviews into its individual words, grouped by review number. Each row in this new dataset represents one word from one review. Using this new data frame, we also quickly calculated the total word count for each review. This would be used later for Term Frequency(TF) calculations. Then came a crucial step: taking this data frame of review words and performing an inner join to our opinion lexicon words. This filters our data frame of reviews broken down by word for only the words that are in our opinion lexicon. This is huge because it saves us from having to worry about removing any personal pronouns, determiners, coordinating conjunctions, and prepositions from the reviews. After performing this step, we take a deeper look at our new data frame using a frequency chart and word cloud:</p>
<p>We can see that the most common word is ‘like’, showing up over 33% more times than the next most common word. Other frequent words include “good”, “well”, “bad”, and “great”. We don’t see any stop words among our most frequent words, indicating a successful dataframe join.</p>
</section>
<section id="tf-idf-and-numericalization" class="level2">
<h2 class="anchored" data-anchor-id="tf-idf-and-numericalization">TF-IDF and Numericalization</h2>
<p>Although our reviews are now filtered to only include words from the lexicon, we still need to remove some more words; those that don’t give us much useful information about the sentiment of the reviews. We have over 5800 unique words from the lexicon in our dataset and utilizing them all as features will be far too computationally expensive.</p>
<p>This is where Term Frequency-Inverse Document Frequency(TF-IDF) comes into play. We use this to perform dimensionality reduction on our dataset and remove words outside of a set bound. For the Term Frequency calculation, rather than finding the # of times a word appears in a review / # of words in the review, we found the # of times each word appeared in the entire dataset / the # of words in the entire dataset. Doing this essentially gives us an average TF for each word and limits us to only one TF-IDF for each unique word rather than 50,000.</p>
<p>The TF-IDF values ranged from 1.1e-5 to about 0.0315. The vast majority of the 5817 lexicon words, almost 5400 words, had a TF-IDF below 0.002. Since only a small portion of our words had a TF-IDF above this, we decided to make this a minimum cutoff. We did not need a maximum cutoff for the TF-IDF since the stop words had already been taken care of. When filtering for words with a TF-IDF &gt;0.002, we are left with 435 words or “important features”.</p>
<p>With these remaining words, we get a count of how many times each word appears in each review, then we pivot this data frame so that each column is one of 435 words, each row is a review number, and each cell value is the frequency of that word in the review. This new numericalized data frame is combined with the columns from our original dataset (excluding the review column) to create our final dataset. 71 of the 50,000 reviews did not contain any of the 435 words so the NA values in those rows were replaced with zeros.</p>
<p>The last step in our preprocessing work was to split our dataset of 50k rows and 437 columns (1 row_num + 1 sentiment + 435 words) into testing and training sets of 25k rows each. We now have balanced, numericalized datasets ready for modeling.</p>
</section>
</section>
<section id="analysis" class="level1">
<h1>Analysis</h1>
<p>With our data now properly filtered and numericalized to the relevant 435 lexicon words, we were able to begin modeling and predicting the sentiment of each review in the testing dataset based on the training dataset. We elected to evaluate four different models: logistic regression, linear discriminant analysis, quadratic discriminant analysis, and a random forest. This gave us an even split between discriminative models and generative models for the purpose of study.</p>
<section id="logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h2>
<p>Logistic regression is a discriminative model often employed for classification problems, much like this one where we are attempting to classify whether a movie review had a “positive” or “negative” sentiment based on the contents of the review.</p>
<p>The first step of employing logistic regression was a slight alteration of the data. We had to convert the “sentiment” variable from a character denoting “positive” and “negative” to an integer – 1 for “positive” and 0 for “negative.” This allowed us to employ the function to actually do the logistic regression.</p>
<p>After conducting the logistic regression on the training data, we were able to apply it to the testing data to predict the sentiment of each review in the testing data. These were the results of the data:</p>
<p>As we can see, the model was better at accurately predicting the positive reviews than the negative. It had an accuracy of 85.9% for the positive reviews, against just an 81.8% accuracy for the negative reviews. Overall, the logistic regression yielded a total accuracy of 83.8%.</p>
<section id="cross-validation-and-regularization" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation-and-regularization">Cross Validation and Regularization</h3>
<p>After conducting the initial logistic regression on the training and testing datasets, we wanted to make sure that our model did not overfit to the training data and can generalize well to other, new data. To do this, we looked to L2 (Ridge) Regularization. To determine the parameter, lambda, which controls the strength of the penalty term, we used 5-fold cross validation. Interestingly, the cv kept selecting the smallest <span class="math inline">\(\lambda\)</span> in our range as the best choice, even as we continued to feed the cv model a lower range. It became clear the the best results/accuracy came from a model with no penalty. In fact, when using 5-fold cv directly on our training dataset with no penalty term, we get an accuracy of 83.9%, slightly higher than the initial model. This tells us that our model is stable and generalizes well across different subsets of data.</p>
<p>The ROC curve for logistic regression shows a relatively large area under the curve, indicating that the model fits well.</p>
</section>
</section>
<section id="linear-discriminant-analysis" class="level2">
<h2 class="anchored" data-anchor-id="linear-discriminant-analysis">Linear Discriminant Analysis</h2>
<p>Linear Discriminant Analysis (LDA) is a generative model, unlike the discriminative logistic model, which studies the joint probability distribution as opposed to the purely conditional probability in generative models.</p>
<p>First, we must make an assumption that the conditional probability of the review sentiment based on the words found in the review follows a multivariate normal distribution with a fixed mean vector and covariance matrix. This step of the process will also be repeated when we conduct Quadratic Discriminant Analysis (QDA) later. However, the main difference between the LDA and the QDA is that here, we will assume that the classes have a common covariance matrix between them.</p>
<p>The methods of conducting LDA are largely the same as the logistic regression, as we use the same data set which alters “positive” and “negative” to 1’s and 0’s, and the process of generating the table is virtually the same by using the training data to predict the testing data.</p>
<p>The accuracy of predicting a positive sentiment on the testing data was 81.9%, and the accuracy of predicting a negative sentiment on the testing data was 85.2%, so compared to the logistic regression, both the true positive rate and true negative rate increased, and the true negative rate was higher than the true positive rate. The overall accuracy of this model is about 83.5%, which is an improvement from the logistic regression model.</p>
<p>This is the Receiver Operating Characteristic (ROC) curve for the LDA model, and what this helps visualize are the true positive and false positive rates. The more bowed out the curve is, the better the model performed in terms of accuracy, and the one of the LDA model is very bowed out, with a large area underneath the curve, so we can assume this model was strong in predicting the sentiment of the review based on the words in the review.</p>
</section>
<section id="quadratic-discriminant-analysis" class="level2">
<h2 class="anchored" data-anchor-id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h2>
<p>Quadratic Discriminant Analysis (QDA), another type of generative model, is closely related to LDA. QDA retains one of the core LDA assumptions that the observations within each class are drawn from a multivariate Gaussian distribution. However, as stated previously, an important distinction that differentiates QDA from LDA is that each class has its own covariance matrix. Typically, QDA is utilized when the training dataset is very large so that the variance of the classifier is not a major concern. QDA serves as a middle ground between non-parametric methods like KNN and linear approaches like LDA and logistic regression. While it assumes a quadratic decision boundary, it still provides more flexibility than the linear methods, giving it the potential to capture more complex relationships<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>After training the QDA model on the training dataset of 25,000 samples, we evaluated its performance on the testing data, which consists of the same amount of samples. The response variable Sentiment was converted into a factor before fitting the model and making predictions, with levels “Positive” and “Negative,” to ensure that it is treated as a binary classification variable.</p>
<p>As we can see from the confusion matrix, the accuracy of predicting a positive sentiment is around 78.3% (True Positives/Total Positives), while the accuracy of predicting a negative sentiment is around 82.3% (True Negatives/Total Negatives). These accuracies are both lower than QDA’s linear counterpart LDA’s TP and TN accuracies. The overall accuracy of the QDA model is 80.2%, slightly less than both Logistic Regression and LDA.</p>
<p>A ROC curve plots the True Positive Rate (TPR) vs False Positive Rate (FPR) at different classification thresholds. “Sensitivity” refers to the TPR, whereas 1 - Specificity refers to the FPR. A side-by-side comparison of the QDA Model ROC to those of Logistic Regression and LDA shows that both models outperform QDA; their curves are more bowed out towards the top-left corner, indicating a higher sensitivity (TPR)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<section id="why-does-logistic-regression-and-lda-outperform-qda" class="level3">
<h3 class="anchored" data-anchor-id="why-does-logistic-regression-and-lda-outperform-qda">Why does Logistic Regression and LDA outperform QDA?</h3>
<p>As we saw from the results, LDA and Logistic Regression slightly outperform QDA. While the difference may be marginal, there are possible explanations that might help us understand the differing accuracies. The assumption of varying covariance matrices for different classes may introduce higher variance, leading to possible overfitting and reduced generalization ability. The comparatively simpler LDA and logistic regression models might find it easier to generalize, underscoring a potential bias-variance tradeoff that favors these models.</p>
</section>
</section>
<section id="random-forest" class="level2">
<h2 class="anchored" data-anchor-id="random-forest">Random Forest</h2>
<p>Decision trees are constructed by repeated splits of subsets of the data (the root node) into two descending subsets (child nodes). A terminal node, or leaf node, ideally occurs when that node is purely made up of one class. By combining hundreds or even thousands of decision trees, we can create a random forest that contains trees with many different subsets of the data. Random forest models are considered one of the more accurate predictive models, although the computing time is quite long and inefficient compared to other models (i.e.&nbsp;this algorithm is computationally expensive).</p>
<p>After splitting the dataset into 50% for training and 50% for testing, we first did Random Search Cross Validation in order to determine some ideal hyperparameters for our model. The results suggested the maximum number of features for each tree be 4.</p>
<p>Our random forest model with the updated hyperparameter had an accuracy of 83.4%, which was roughly 2% better than the model with default hyperparameters. Shown below are the results of the model and its ROC curve.</p>
<p>The true positive rate (sensitivity) was 85.1%, and the false positive rate (specificity) was 81.8%.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>All four chosen models performed generally well, with each exceeding the threshold of 80% accuracy. Logistic Regression performed the best. LDA came in second, narrowly returning a higher accuracy than the random forest. However, the 0.1% difference between the two is not large enough to conclusively say that one is better than the other. QDA came in last, with an accuracy just narrowly crossing the 80% threshold. The lower accuracy of QDA tells us that varying covariance matrices between classes/sentiments was not a beneficial assumption to make. The QDA model likely overfit to the training data, resulting in under-performance on the testing dataset. The accuracy of LDA, on the other hand suggests that it was relatively safe to assume that observations within each class come from a multivariate Gaussian distribution.</p>
<p>Based on the results and accuracy, we would definitely recommend a Logistic Regression model. The next best models, LDA and Random Forest, still had lower accuracies and are more computationally expensive in this case. This could make a significant difference if these models were being put into production and/or used on a much larger dataset.</p>
<section id="future-analysis" class="level2">
<h2 class="anchored" data-anchor-id="future-analysis">Future Analysis</h2>
<p>While we did end up evaluating four different modeling techniques, future iterations of this study could include a K nearest neighbors analysis, or perform K-fold cross validation on the models other than Logistic Regression and Random Forest. However, cross validation would have made the models even more computationally expensive. In addition, KNN likely would not have performed as well as some of the models we implemented. KNN is a very flexible model and it likely would have overfit to the training data like QDA.</p>
<p>Once again, our models performed fairly well, especially considering that we only used an opinion lexicon and term frequencies to numericalize our reviews. If we were looking to improve our accuracy, possibly north of 90%, we could have also accounted for the context surrounding words. This would help us determine if a word really reflected positive or negative sentiment. In addition, we could have taken punctuation, such exclamation marks, and capitalized words into account. Although relatively uncommon and difficult to detect, the presence of sarcasm could also be sought out. This would also make us reconsider the true sentiment a word or sentence displays.</p>
<p>Lastly, keeping more words from our lexicon would have helped us in improving the accuracy of our models. Reducing our unique words to 435 did ensure that our models ran more efficiently but the TF-IDF cutoff used to make that selection was arbitrarily selected. This reduction also led to some reviews no longer having any words that were “important features”. Keeping more words, at least enough that every review has a word, would be a good decision if we were to perform this analysis again.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Lexicon was provided by professor and can be found <a href="https://www.kaggle.com/datasets/nltkdata/opinion-lexicon"><strong>here</strong></a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Source: ISLR Textbook pg. 151<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Source: <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc"><strong>Classification: ROC curve and AUC</strong></a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>© 2024, Odeya Russo</p>
</div>
  </div>
</footer>




</body></html>